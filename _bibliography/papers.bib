---
---
@inproceedings{dubowski2024subgroup,
	title = {{Subgroup Harm Assessor: Identifying Potential Fairness-Related Harms and Predictive Bias}},
	author = {Adam Dubowski and Hilde Weerts and Anouk Wolters and Mykola Pechenizkiy},
	year = {2024},
	booktitle = {Machine Learning and Knowledge Discovery in Databases: Applied Data Science and Demo Track},
	publisher = {Springer Nature Switzerland},
	bibtex_show={true},
	theme={algorithmic fairness},
}

@article{weerts2019case,
	title = {{Case-Based Reasoning for Assisting Domain Experts in Processing Fraud Alerts of Black-Box Machine Learning Models}},
	author = {Weerts, Hilde Jacoba Petronella and van Ipenburg, Werner and Pechenizkiy, Mykola},
	year = {2019},
	journal = {Proceedings of KDD Workshop on Anomaly Detection in Finance (KDD-ADF ’19)},
	bibtex_show={true},
	theme={algorithmic fairness},

}


@article{weerts2019human,
	title = {{A Human-Grounded Evaluation of Shap for Alert Processing}},
	author = {Weerts, Hilde Jacoba Petronella and van Ipenburg, Werner and Pechenizkiy, Mykola},
	year = {2019},
	journal = {Proceedings of KDD Workshop on Explainable AI (KDD-XAI ’19)},
	bibtex_show={true},
}

@article{weerts2022does,
	title = {{Are There Exceptions to Goodhart's Law? On the Moral Justification of Fairness-Aware Machine Learning}},
	author = {Weerts, Hilde and Royakkers, Lamb{\`e}r and Pechenizkiy, Mykola},
	year = {2022},
	journal = {arXiv preprint arXiv:2202.08536},
	bibtex_show={true},
	theme={algorithmic fairness},
}


@inproceedings{weerts2022teaching,
	title = {{Teaching Responsible Machine Learning to Engineers}},
	author = {Weerts, Hilde Jacoba Petronella and Pechenizkiy, Mykola},
	year = {2022},
	booktitle = {Proceedings of the Second Teaching Machine Learning and Artificial Intelligence Workshop},
	pages = {40--45},
	editor = {Kinnaird, Katherine M. and Steinbach, Peter and Guhr, Oliver},
	volume = {170},
	series = {Proceedings of Machine Learning Research},
	publisher = {PMLR},
	pdf = {https://proceedings.mlr.press/v170/weerts22a/weerts22a.pdf},
	bibtex_show={true},
	abstract={With the increasing application of machine learn- ing in practice, there is a growing need to incor- porate ethical considerations in engineering cur- ricula. In this paper, we reflect upon the develop- ment of a course on responsible machine learning for undergraduate engineering students. We found that technical material was relatively easy to grasp when it was directly linked to prior knowledge on machine learning. However, it was non-trivial for engineering students to make a deeper con- nection between real-world outcomes and ethical considerations such as fairness. Moving forward, we call upon educators to focus on the develop- ment of realistic case studies that invite students to interrogate the role of an engineer.},
}


@inproceedings{weerts2023algorithmic,
	title = {{Algorithmic Unfairness Through the Lens of EU Non-Discrimination Law: Or Why the Law Is Not a Decision Tree}},
	author = {Weerts, Hilde and Xenidis, Rapha\"{e}le and Tarissan, Fabien and Olsen, Henrik Palmer and Pechenizkiy, Mykola},
	year = {2023},
	isbn = {9798400701924},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3593013.3594044},
	booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
	pages = {805–816},
	numpages = {12},
	keywords = {machine learning, artificial intelligence, algorithmic fairness, EU non-discrimination law},
	location = {Chicago, IL, USA},
	series = {FAccT '23},
	bibtex_show={true},
	theme={algorithmic fairness},
}


@article{weerts2023fairlearn,
	title = {{Fairlearn: Assessing and Improving Fairness of AI Systems}},
	author = {Hilde Weerts and Miroslav Dudík and Richard Edgar and Adrin Jalali and Roman Lutz and Michael Madaio},
	year = {2023},
	journal = {Journal of Machine Learning Research},
	volume = {24},
	number = {257},
	pages = {1--8},
	abstract={Fairlearn is an open source project to help practitioners assess and improve fairness of artificial intelligence (AI) systems. The associated Python library, also named fairlearn, supports evaluation of a model’s output across affected populations and includes several algorithms for mitigating fairness issues. Grounded in the understanding that fairness is a sociotechnical challenge, the project integrates learning resources that aid practitioners in considering a system’s broader societal context.},
	pdf={https://www.jmlr.org/papers/volume24/23-0389/23-0389.pdf},
	bibtex_show={true},
	theme={algorithmic fairness},

}


@inproceedings{weerts2023look,
	title = {{Look and You Will Find It: Fairness-Aware Data Collection Through Active Learning.}},
	author = {Weerts, Hilde J. P. and Theunissen, Ren{\'e}e and Willemsen, Martijn C},
	year = {2023},
	booktitle = {Proceedings of the 7th Intl. Worksh. \& Tutorial on Interactive Adaptive Learning},
	location = {Torino, Italy},
	series = {IAL@ECML-PKDD’23},
	pages = {74--88},
	bibtex_show={true},
	theme={algorithmic fairness},
}


@article{weerts2024can,
	title = {{Can Fairness Be Automated? Guidelines and Opportunities for Fairness-Aware AutoML}},
	author = {Weerts, Hilde and Pfisterer, Florian and Feurer, Matthias and Eggensperger, Katharina and Bergman, Edward and Awad, Noor and Vanschoren, Joaquin and Pechenizkiy, Mykola and Bischl, Bernd and Hutter, Frank},
	year = {2024},
	journal = {Journal of Artificial Intelligence Research},
	volume = {79},
	pages = {639--677},
	bibtex_show={true},
	theme={algorithmic fairness},
}


@inproceedings{weerts2024neutrality,
	author = {Weerts, Hilde and Xenidis, Rapha\"{e}le and Tarissan, Fabien and Olsen, Henrik Palmer and Pechenizkiy, Mykola},
	title = {The Neutrality Fallacy: When Algorithmic Fairness Interventions are (Not) Positive Action},
	year = {2024},
	isbn = {9798400704505},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3630106.3659025},
	doi = {10.1145/3630106.3659025},
	abstract = {Various metrics and interventions have been developed to identify and mitigate unfair outputs of machine learning systems. While individuals and organizations have an obligation to avoid discrimination, the use of fairness-aware machine learning interventions has also been described as amounting to ‘algorithmic positive action’ under European Union (EU) non-discrimination law. As the Court of Justice of the European Union has been strict when it comes to assessing the lawfulness of positive action, this would impose a significant legal burden on those wishing to implement fair-ml interventions. In this paper, we propose that algorithmic fairness interventions often should be interpreted as a means to prevent discrimination, rather than a measure of positive action. Specifically, we suggest that this category mistake can often be attributed to neutrality fallacies: faulty assumptions regarding the neutrality of (fairness-aware) algorithmic decision-making. Our findings raise the question of whether a negative obligation to refrain from discrimination is sufficient in the context of algorithmic decision-making. Consequently, we suggest moving away from a duty to ‘not do harm’ towards a positive obligation to actively ‘do no harm’ as a more adequate framework for algorithmic decision-making and fair ml-interventions.},
	booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
	pages = {2060–2070},
	numpages = {11},
	keywords = {EU law, algorithmic decision-making, discrimination, positive action},
	location = {Rio de Janeiro, Brazil},
	series = {FAccT '24},
	bibtex_show={true},
	theme={algorithmic fairness},

}


@inproceedings{weerts2024unlawful,
	title = {{Unlawful Proxy Discrimination: A Framework for Challenging Inherently Discriminatory Algorithms}},
	author = {Weerts, Hilde and Kelly-Lyth, Aislinn and Binns, Reuben and Adams-Prassl, Jeremias},
	year = {2024},
	booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
	doi = {10.1145/3630106.3659010},
	bibtex_show={true},
	theme={algorithmic fairness},
}
